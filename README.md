# SKN06-2nd-4Team

SKN6ê¸° 2nd ë‹¨ìœ„ í”„ë¡œì íŠ¸ - ê³µì¸ìš©, ê¹€ë™ëª…, ë°•ìœ ë‚˜, ì„ì—°ê²½

## ê°€ë””ì–¸ì¦ˆ ì˜¤ë¸Œ ë…ì‚°

### íŒ€ì›

| ê³µì¸ìš©                                                                                                                                                                                                                                                                                                                                                                        | ê¹€ë™ëª…                                                                                                                                                                                                                                                                                                                    | ë°•ìœ ë‚˜                                                                                                                                                                                                                             | ì„ì—°ê²½                                                                                                                                                                                                                                                                                           |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| <img src="https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAxODA0MTFfMTM0%2FMDAxNTIzNDU0NzkwNDE5.gEK35xFgCX9vYBn5oOJyWuh2e5pbuhlSbfdl6poV5uEg.Mv_Dfnxo30cnaT6L6CRO1qnHuhw-w2-IOrbCvdavhJ8g.JPEG.hong5395%2F3e9932eb44c7e5d95e3380f0b3850a10849e64a53fc218b5f937de3f8aa32c7d179cdaa4ff41.jpg&type=sc960_832" alt="image" width="200" height="250"/> | <img src="https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAxNzAzMDVfMTA2%2FMDAxNDg4Njk1MjMzMzM5.n8IyNuI2bfb9ahCzK5BuXarECmC0kAgwXAQ_VqnVfvkg.YVo_NXVr0Yum_arFadeksjm5EK2llgXS7c5_gdAAyk0g.JPEG.herotime01%2Fmms411-r9_shop1_151309.jpg&type=sc960_832" alt="image" width="200" height="250"/> | <img src="https://search.pstatic.net/sunny/?src=https%3A%2F%2Fi.namu.wiki%2Fi%2FHTd0cQVU-2HsObW-meRcGxERbzgr80e3y0K2IkUPVuAtCAQgoN684suvdC3B3vAr6G_lT_XJk4j5k7l-_7sLbg.webp&type=sc960_832" alt="image" width="200" height="250"/> | <img src="https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAxODA1MjVfMTMz%2FMDAxNTI3MTk1NDA2NDY4.52sHvf5xhDFsa547Q0XppzIAaz_LuXEm1vIkHcXev8Ag.dxIsd79lNL_5QekTes5_Agf4EzveLb7L1Ub-EHP738Ag.JPEG.loyh%2FDSC01289.JPG&type=a340" alt="image" width="200" height="250"/> |
| ML                                                                                                                                                                                                                                                                                                                                                                            | ML, ë°œí‘œ                                                                                                                                                                                                                                                                                                                  | ML, Streamlit                                                                                                                                                                                                                      | ML                                                                                                                                                                                                                                                                                               |

</br>

# ğŸ’³ ì‹ ìš©ì¹´ë“œ ì´ìš© ê³ ê° - ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ ğŸ’³

### âœ”ï¸ ê°œë°œ ê¸°ê°„

2024.11.14 ~ 2024.11.15(ì´ 2ì¼)

### âœ”ï¸ ê°œìš”

ê¸ˆìœµ ì„œë¹„ìŠ¤ ì‹œì¥ì—ì„œ ê³ ê° ì´íƒˆ ë°©ì§€ëŠ” ìˆ˜ìµì„± ìœ ì§€ì™€ ê²½ìŸë ¥ ê°•í™”ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì´ë‹¤. ì‹ ìš©ì¹´ë“œ ì‚¬ìš© ê³ ê°ì˜ ì´íƒˆ íŒ¨í„´ì„ ì´í•´í•˜ê³ , íš¨ê³¼ì ì¸ ê³ ê° ìœ ì§€ ì „ëµì„ ì„¸ìš°ê¸° ìœ„í•´ ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ í†µí•´ ê¸ˆìœµ ê¸°ê´€ì€ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ê°•í™”í•˜ê³ , ê³ ê° ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤.

### âœ”ï¸ ëª©í‘œ

ì‹ ìš©ì¹´ë“œ ì‚¬ìš© ê³ ê°ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì´íƒˆ ê°€ëŠ¥ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œ

</br>

#### âœ”ï¸ Stacks

![Discord](https://img.shields.io/badge/discord-5865F2?style=for-the-badge&logo=discord&logoColor=white)

![Python](https://img.shields.io/badge/python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Numpy](https://img.shields.io/badge/NumPy-013243?style=flat-square&logo=NumPy&logoColor=white)
![pandas](https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
<img src="https://img.shields.io/badge/scikitlearn-%23F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white"/>

![Streamlit](https://img.shields.io/badge/streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
</br>

![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=Git&logoColor=white)
![Github](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white)

#### âœ”ï¸ Requirements

streamlit == 1.39.0 <br/>
pymysql == 1.1.1 <br/>
pandas == 2.2.3 <br/>
openpyxl == 1.1.0 <br/>
sqlalchemy == 2.0.35 <br/>
configparser == 7.1.0 <br/>
matplotlib == 3.9.2 <br/>
xlrd == 2.0.1 <br/>
seaborn == 0.13.2 <br/>
joblib == 1.3.2 <br/>
scikit-learn == 1.3.1 <br/>
numpy == 1.26.4 <br/>
xgboost == 1.7.6 <br/>

## ë°ì´í„° ì¤€ë¹„ ë° ë¶„ì„

### âœ”ï¸ Column ì •ì˜

[Google ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë³´ê¸°](https://docs.google.com/spreadsheets/d/1PvMto9SCOenoNsXg_mjzhMyAeArdpVEP5e5ZlOuftFI/edit?usp=sharing)

| Column ì´ë¦„             | Description                | Feature Value                                                                               | ë¹„ê³                                                                                                                                                     |
| ----------------------- | -------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| CLIENTNUM               | ê³ ê° ë²ˆí˜¸                  | n                                                                                           | ì‚­ì œ í•„ìš”                                                                                                                                               |
| churn                   | ê²°ê³¼ ê°’(churn)             | "Existing", "Attrited"                                                                      | - Encoding â†’ Label Encoding                                                                                                                             |
| age                     | ë‚˜ì´                       | 26~57                                                                                       |                                                                                                                                                         |
| gender                  | ì„±ë³„                       | M, F                                                                                        | - Encoding â†’ Label Encoding                                                                                                                             |
| dependent_cnt           | ë¶€ì–‘ ê°€ì¡±ìˆ˜                | 0, 1, 2, 3, 4, 5                                                                            |                                                                                                                                                         |
| education_level         | í•™ë ¥                       | "Graduate", "High School", "Unknown", "Uneducated", "College", "Post-Graduate", "Doctorate" | - ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•„ìš”: Unknown(1519ê°œ/0.149) â†’ ìµœë¹ˆê°’ <br> - graduateì˜ ë¹„ìœ¨ì´ ë†’ìŒ<br>- Encoding - ìˆœì„œì˜ ì˜ë¯¸ê°€ ìˆì–´ ë³´ì„ â†’ ìˆœì„œ ì¸ì½”ë”© (Ordinal Encoding)|
| marital_status          | ê²°í˜¼ ì—¬ë¶€                  | "Married", "Single", "Unknown", "Divorced"                                                  | - ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•„ìš”: Unknown(749ê°œ/0.07) â†’ ìµœë¹ˆê°’<br>- marriedì˜ ë¹„ìœ¨ì´ ë†’ìŒ<br>- Encoding â†’ One-Hot Encoding                                            |
| income_category         | ì†Œë“ ìˆ˜ì¤€(ë²”ì£¼)            | "Unknown", "Less than $40K", "$40K - $60K", "$80K - $120K", "$60K - $80K", "$120K +"        | - ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•„ìš”: Unknown(1112ê°œ/0.109) â†’ ë¹„ë¡€ë°°ë¶„<br>- Encoding - ìˆœì„œì˜ ì˜ë¯¸ê°€ ìˆì–´ ë³´ì„ â†’ ìˆœì„œ ì¸ì½”ë”© (Ordinal Encoding)                           |
| card_category           | ì¹´ë“œ ì¢…ë¥˜(ë²”ì£¼)            | "Blue", "Silver", "Gold", "Platinum"                                                        | - Encoding â†’ One-Hot Encoding                                                                                                                           |
| card_usage_period       | ì¹´ë“œ ì‚¬ìš© ê¸°ê°„             | n                                                                                           | - max : 56 > min : 13<br>- 0.15 ê¸°ì¤€ìœ¼ë¡œ íƒˆë½                                                                                                           |
| account_cnt             | ê³„ì¢Œ ìˆ˜                    | 1, 2, 3, 4, 5, 6                                                                            |                                                                                                                                                         |
| inactive_month_in_year  | ì—°ë‚´ ê³„ì¢Œ ë¹„í™œì„± ê¸°ê°„      | 0, 1, 2, 3, 4, 5, 6                                                                         |                                                                                                                                                         |
| visit_cnt_in_year       | ì—°ê°„ ì€í–‰ ë°©ë¬¸ ìˆ˜          | 0, 1, 2, 3, 4, 5, 6                                                                         |                                                                                                                                                         |
| credit_limit            | ì‹ ìš© í•œë„                  | n                                                                                           | - max: 34516.0 > min: 1438.3                                                                                                                            |
| revolving_balance       | ì”ê¸ˆ                       | n                                                                                           | - max: 2517 > min: 1438.3                                                                                                                               |
| avg_remain_credit_limit | í‰ê·  ì”ì—¬ ì‹ ìš© í•œë„        | n                                                                                           | - max : 34516.0 > min: 3.0                                                                                                                        |
| total_amt_change_Q4_Q1  | ì—°ê°„ ê±°ë˜ì•¡ ë³€í™”ìœ¨(Q4/Q1)  | n                                                                                           | - max : 3.397 / min: 0<br>                                                                                                                              |
| total_trans_amt         | ì´ ê±°ë˜ ê¸ˆì•¡               | n                                                                                           | - max : 18484 / min: 510                                                                                                                                |
| total_trans_cnt         | ì´ ê±°ë˜ íšŸìˆ˜               | n                                                                                           | - max : 139 / min : 0<br>                                                                                                                               |
| total_cnt_change_Q4_Q1  | ì´ ê±°ë˜ íšŸìˆ˜ ë³€í™”ìœ¨(Q4/Q1) | n                                                                                           |                                                                                                                                                         |
| avg_utilization_ratio   | ì¹´ë“œ í•œë„ ëŒ€ë¹„ ì”ì•¡ì˜ ë¹„ìœ¨ | 0 <= n <= 1                                                                                 | - 0 ~ 1 ì‹¤ìˆ˜<br>- '0' ì˜ ë¹„ìœ¨ì´ ë†’ìŒ                                                                                                                    |

### âœ”ï¸ EDA(íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)

![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/EDA.png)

<br/>

## ë°ì´í„° ì „ì²˜ë¦¬
### âœ”ï¸ 1. ì¹¼ëŸ¼ëª… ìˆ˜ì • ë° ì†Œë¬¸ìí™”
```
rename_columns = {
        'Attrition_Flag': 'churn',
        'Customer_Age' : 'age',
        'Dependent_count' : 'dependent_cnt',
        'Months_on_book' : 'card_usage_period',
        'Total_Relationship_Count' : 'account_cnt',
        'Months_Inactive_12_mon' : 'inactive_month_in_year',
        'Contacts_Count_12_mon' : 'visit_cnt_in_year',
        'Total_Revolving_Bal' : 'revolving_balance',
        'Avg_Open_To_Buy' : 'avg_remain_credit_limit',
        'Total_Amt_Chng_Q4_Q1' : 'total_amt_change_q4_q1',
        'Total_Trans_Ct' : 'total_trans_cnt',
        'Total_Ct_Chng_Q4_Q1' : 'total_cnt_change_q4_q1'
    }
data.rename(columns=rename_columns, inplace=True)
data.columns = data.columns.str.lower()
```
### âœ”ï¸ 2. ë¶ˆí•„ìš” ì¹¼ëŸ¼ ì‚­ì œ
> clientnum : íšŒì›ë²ˆí˜¸
> 
> naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1
>
> naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2
> 
```
data = data.drop(
    columns=[
        'clientnum',
        'naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1',
        'naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2'
    ], 
    inplace=True
)
```
### âœ”ï¸ 3. ê²°ê³¼ê°’ 'churn' mapping
> Existing Customer: 0
>
> Attrited Customer: 1 (ì´íƒˆ)
>
```
data['churn'] = data['churn'].map({"Existing Customer": 0, "Attrited Customer": 1})
```
### âœ”ï¸ 4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬

â­ï¸ 3ê°œì˜ ë¬¸ìì—´ ì¹¼ëŸ¼ì—ì„œ 'Unknown' ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ëë‹¤. ë‹¤ì–‘í•œ ì²˜ë¦¬ ë°©ë²• ì¤‘ ì‚­ì œë¥¼ ê³ ë ¤í•˜ê¸°ë„ í–ˆì§€ë§Œ, ì‚­ì œí•  ê²½ìš° ë°ì´í„° ì†ì‹¤ì´ ë§ì•„ì§ˆ ê²ƒ ê°™ì•„ **ëŒ€ì²´** ë°©ë²•ì„ ì„ íƒí–ˆë‹¤.

| education_level                                                                                                                             | marital_status                                                                                                                              | income_category                                                                                                                             |
| ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| ![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%B8%A1%EC%B9%98%20%ED%95%99%EB%B2%8C.png) | ![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%B8%A1%EC%B9%98%20%EA%B2%B0%ED%98%BC.png) | ![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%B8%A1%EC%B9%98%20%EC%9E%90%EC%82%B0.png) |
| SimpleImputer(**ìµœë¹ˆê°’**)                                                                                                                   | SimpleImputer(**ìµœë¹ˆê°’**)                                                                                                                   | ì‚¬ìš©ì ì •ì˜ imputer(**ê°€ì¤‘ëŒ€ì²´**)                                                                                                           |
| unkownì˜ ë¹„ìœ¨ì´ ë‚˜ë¨¸ì§€ì— ë¹„ì— ë†’ì§€ ì•ŠìŒ                                                                                                     | unkownì˜ ë¹„ìœ¨ì´ ë‚˜ë¨¸ì§€ì— ë¹„ì— ë†’ì§€ ì•ŠìŒ                                                                                                     | unkownì˜ ë¹„ìœ¨ì´ ë‚˜ë¨¸ì§€ì— ë¹„ì— ë†’ìŒ                                                                                                          |
| Graduateê°€ ê°€ì¥ ë§ì€ ë¹„ìœ¨(30.89%)ì„ ì°¨ì§€                                                                                                    | Marriedê°€ ê°€ì¥ ë†’ì€ ë¹„ìœ¨(46.28%)ì„ ì°¨ì§€                                                                                                     | ê°ê° ë‚˜ë¨¸ì§€ ìë£Œì˜ ë¹„ìœ¨ì— ë”°ë¼ ëœë¤ìœ¼ë¡œ ë¶„ë°°                                                                                                |

</br>
ğŸ‘‰ğŸ» ìš°ë¦¬ê°€ '<b>ê°€ì¤‘ëŒ€ì²´</b>'ë¥¼ ìœ„í•´ ì •ì˜í•œ Imputer
</br>
</br>

```python
class ProportionalImputer(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns
        self.fill_values = {}

    def fit(self, X, y=None):
        for column in self.columns:
            value_counts = X[column].value_counts(normalize=True)
            self.fill_values[column] = (value_counts.index, value_counts.values)
        return self

    def transform(self, X):
        X = X.copy()
        for column in self.columns:

            nan_count = X[column].isna().sum()
            if nan_count > 0:
                fill_values = np.random.choice(
                    self.fill_values[column][0], size=nan_count, p=self.fill_values[column][1]
                )
                X.loc[X[column].isna(), column] = fill_values
        return X
```

### âœ”ï¸ 5. ì´ìƒì¹˜ í™•ì¸ ë° ê° ì´ìƒì¹˜ ì²˜ë¦¬

#### ì´ìƒì¹˜ í™•ì¸
![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/boxplot.png)
</br>

```
def get_normal_range(data, whis=1.5):
    """
    IQR ê¸°ë°˜ìœ¼ë¡œ ì •ìƒë²”ìœ„ ì¡°íšŒ ë©”ì†Œë“œ
    parameter1
        data: ì¡°íšŒí•  ëŒ€ìƒ ë°ì´í„°
        whis: IQRì— ëª‡ë°°ë¥¼ ê·¹ë‹¨ì¹˜ ê³„ì‚°ì— ì‚¬ìš©í•  ì§€ ë¹„ìœ¨. rateë¥¼ í¬ê²Œí•˜ë©´ outlierë²”ìœ„ë¥¼ ë„“ê²Œ ì¡ëŠ”ë‹¤. ì‘ê²Œ ì£¼ë©´ ë²”ìœ„ë¥¼ ì¢ê²Œ ì¡ëŠ”ë‹¤.
    return
        tuple: (lower_bound, upper_bound) - ì •ìƒë²”ìœ„ì˜ í•˜í•œê°’ê³¼ ìƒí•œê°’
    """
    q1 = np.nanquantile(data, q=0.25)
    q3 = np.nanquantile(data, q=0.75)
    IQR = q3 - q1
    lower_bound = q1 - IQR * whis
    upper_bound = q3 + IQR * whis
    return lower_bound, upper_bound
```

#### â‘  age
> ë‚˜ì´
>
> ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ê°’ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.
>
>    - ì •ìƒ ë²”ìœ„ë¥¼ ë„˜ì–´ê°„ ê°’ë“¤ì˜ ê°œìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ê°™ì€ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì„œ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤.

```
data['total_trans_cnt'].describe()
data['total_trans_cnt'].plot(kind='hist', bins=10)
low, high = get_normal_range(data['total_trans_cnt'], whis=1.5)
print(low, high)
data.query('total_trans_cnt > @high').shape
```
</br>

#### â‘¡ total_trans_cnt
> ì´ ê±°ë˜ íšŸìˆ˜
>
> ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ê°’ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.
>
> - ì •ìƒ ë²”ìœ„ë¥¼ ë„˜ì–´ê°„ ê°’ë“¤ì˜ ê°œìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ê°™ì€ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì„œ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤.

```
np.round(data['total_trans_cnt'].describe(), 2)
data['total_trans_cnt'].plot(kind='hist', bins=10)
low, high = get_normal_range(data['total_trans_cnt'], whis=1.5)
print(low, high)
data.query('total_trans_cnt > @high').shape
```

â­ï¸ IQRì„ ì‚¬ìš©í•´ ì´ìƒì¹˜ë¥¼ í™•ì¸í•œ ê²°ê³¼, ì¼ë¶€ ì´ìƒì¹˜ë¡œ ì¶”ì •ë˜ëŠ” ë°ì´í„°ë¥¼ ë°œê²¬í–ˆë‹¤. ê·¸ì¤‘ ["age", "total_trans_cnt"] ì¹¼ëŸ¼ì—ì„œ ê°ê° 2ê°œì˜ ê·¹ë‹¨ì¹˜ ë°ì´í„°ê°€ ê²°ê³¼ì— ê±°ì˜ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ íŒë‹¨ë˜ì–´ ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ ê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆë‹¤.
</br>
</br>


### âœ”ï¸ 6. Feature Engineering

ë°ì´í„° íŠ¹ì§•ë³„ ì¸ì½”ë”© ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤. - 3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì ìš©

â‘  ë¼ë²¨ ì¸ì½”ë”©(Label Encoding)
   > 'gender'
   >
   > ì´ì§„ ë³€ìˆ˜ì˜ ê²½ìš° ëª¨ë¸ ì„±ëŠ¥ì— í° ì°¨ì´ê°€ ì—†ìœ¼ë¯€ë¡œ, ê°„ë‹¨íˆ ë¼ë²¨ ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í•¨.
   > 

â‘¡ ìˆœì„œ ì¸ì½”ë”© (Ordinal Encoding)
   > 'education_level', 'income_category'
   >
   > í•™ë ¥ê³¼ ì†Œë“ê³¼ ê´€ë ¨ëœ ìë£ŒëŠ” ìë£ŒëŸ‰ì´ ì•„ë‹Œ í•´ë‹¹ indexë¡œ ìˆœì„œë¥¼ ê²°ì •í•˜ê¸° ìœ„í•¨.
   >

â‘¢ ì›í•« ì¸ì½”ë”©(One-Hot encoding)
   > 'marital_status', 'card_category'
   >
   > ìˆœì„œê°€ ì—†ê³  ê° ê°’ì´ ë…ë¦½ì ì¸ ë²”ì£¼í˜• ë°ì´í„°ìœ¼ë¡œì„œ ìˆœì„œë‚˜ í¬ê¸° ì •ë³´ ì—†ì´ ê°ê° ë…ë¦½ì ì¸ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜ë˜ë¯€ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ë” ì˜ í•´ì„ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  ë³´ì•„ OneHot ì¸ì½”ë”© í•˜ê¸°ë¡œ ê²°ì •.
   >
   > models/ohe_encoder.pkl ë¡œ ì €ì¥
   > 

<br/>

```python
class LabelEncoderTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.encoder = LabelEncoder()

    def fit(self, X, y=None):
        self.encoder.fit(X)
        return self

    def transform(self, X):
        return self.encoder.transform(X).reshape(-1, 1)  # 1D ë°°ì—´ì„ 2Dë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜


class OrdinalEncoderTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, categories=[]):
        print(categories)
        self.encoder = OrdinalEncoder(categories=categories)

    def fit(self, X, y=None):
        self.encoder.fit(X)
        return self

    def transform(self, X):
        return self.encoder.transform(X)  
```
â­ï¸ ìœ„ì™€ ê°™ì´ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ê³  íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±
```
education_categories = [["Uneducated", "High School", "College", "Graduate", "Post-Graduate", "Doctorate"]]
income_categories = [['Less than $40K', '$120K +', '$40K - $60K', '$60K - $80K', '$80K - $120K']]

encoder = ColumnTransformer(
    [
        ('gender_encoder', LabelEncoderTransformer(), [5]), # gender
        ('education_encoder', OrdinalEncoder(categories=education_categories), [3]), # education_level
        ('income_encoder', OrdinalEncoder(categories=income_categories), [2]), # income_category
        ('marital_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [4]), # marital_status
        ('card_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [7]) # card_category
    ], remainder='passthrough'
)

```
### ğŸ“Œ ì „ì²˜ë¦¬ ì •ë¦¬
> ê²°ì¸¡ì¹˜ ì²˜ë¦¬
> 
> - income_category: ë¹„ìœ¨ì— ë”°ë¥¸ ëŒ€ì¹˜
> - education_level, marital_status: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
>
> ì´ìƒì¹˜ ì²˜ë¦¬
> - age, total_trans_ct : ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ ê°’ìœ¼ë¡œ ëŒ€ì²´
>
> encoding
> - gender : ë¼ë²¨ ì¸ì½”ë”©(Label Encoding)
> - education_level : ìˆœì„œ ì¸ì½”ë”© (Ordinal Encoding)
> - marital_status, card_category : ì›í•« ì¸ì½”ë”©(One-Hot encoding)

### âœ”ï¸ DataLoad í•¨ìˆ˜
```
%%writefile dataloader.py

import pandas as pd

def load_dataset():
    # ë°ì´í„° load
    data = pd.read_csv("data/credit_card_churn.csv", na_values="Unknown")

    # ì»¬ëŸ¼ëª… ë³€ê²½
    rename_columns = {
        "Attrition_Flag": "churn",
        "Customer_Age": "age",
        "Dependent_count": "dependent_cnt",
        "Months_on_book": "card_usage_period",
        "Total_Relationship_Count": "account_cnt",
        "Months_Inactive_12_mon": "inactive_month_in_year",
        "Contacts_Count_12_mon": "visit_cnt_in_year",
        "Total_Revolving_Bal": "revolving_balance",
        "Avg_Open_To_Buy": "avg_remain_credit_limit",
        "Total_Amt_Chng_Q4_Q1": "total_amt_change_q4_q1",
        "Total_Trans_Ct": "total_trans_cnt",
        "Total_Ct_Chng_Q4_Q1": "total_cnt_change_q4_q1",
    }
    data.rename(columns=rename_columns, inplace=True)
    # ì»¬ëŸ¼ëª… ì†Œë¬¸ìë¡œ ë³€ê²½
    data.columns = data.columns.str.lower()

    ## ì»¬ëŸ¼ ì‚­ì œ
    data.drop(
        columns=[
            "clientnum",
            "naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1",
            "naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2",
        ],
        inplace=True,
    )

    X = data.drop(columns="churn")
    y = data["churn"]
    y = data['churn'].map({"Existing Customer": 0, "Attrited Customer": 1})
    

    return X, y
```
### âœ”ï¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±
#### ì‚¬ìš©ì ì „ì²˜ë¦¬ê¸° ìƒì„±
>fit() ì€ ì…ë ¥ë°›ì€ ë°ì´í„° X ì™€ y ë¥¼ ì‚¬ìš©í•´ ë³€í™˜í•  ë•Œ ì‚¬ìš©í•  ê°’ì„ ì°¾ì•„ self ì— attributeë¡œ ì €ì¥í•œë‹¤.
>
>transform() ì€ fit() ì—ì„œ ì°¾ì€ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤.
```
%%writefile preprocessing.py

import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder

# ì´ìƒì¹˜ ì²˜ë¦¬
# age, total_trans_coun ì „ì²˜ë¦¬ì— ì ìš©í•  transformer í´ë˜ìŠ¤
## - ì •ìƒë²”ìœ„ ìµœëŒ€ê°’, ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´

class OutlierTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, whis=1.5):
        self.whis = whis
    
    def fit(self, X, y=None):
        q1 = np.nanquantile(X, q=0.25)
        q3 = np.nanquantile(X, q=0.75)
        IQR = q3 - q1
        self.lower_bound = q1 - IQR * self.whis
        self.upper_bound = q3 + IQR * self.whis
        return self
    
    def transform(self, X, y=None):
        X_transformed = np.where(X < self.lower_bound, self.lower_bound, X)
        X_transformed = np.where(X_transformed > self.upper_bound, self.upper_bound, X_transformed)
        return X_transformed


# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
class ProportionalImputer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.proportions = {}
    
    def fit(self, X, y=None):
        # ê° ì—´ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ì €ì¥
        for column in X.columns:
            counts = X[column].value_counts(normalize=True, dropna=True)
            self.proportions[column] = counts
        return self
    
    def transform(self, X):
        X = X.copy()
        for column, probs in self.proportions.items():
            # ê²°ì¸¡ì¹˜ ìœ„ì¹˜ ì°¾ê¸°
            missing_mask = X[column].isna()
            if missing_mask.any():
                # ë¹„ìœ¨ì— ë”°ë¼ ëœë¤í•˜ê²Œ ê°’ ì±„ìš°ê¸°
                X.loc[missing_mask, column] = np.random.choice(
                    probs.index, size=missing_mask.sum(), p=probs.values
                )
        return X 
    
class LabelEncoderTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.encoder = LabelEncoder()

    def fit(self, X, y=None):
        self.encoder.fit(X)
        return self

    def transform(self, X):
        return self.encoder.transform(X).reshape(-1, 1)  # 1D ë°°ì—´ì„ 2Dë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜


class OrdinalEncoderTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, categories=[]):
        print(categories)
        self.encoder = OrdinalEncoder(categories=categories)

    def fit(self, X, y=None):
        self.encoder.fit(X)
        return self

    def transform(self, X):
        return self.encoder.transform(X) 
```
#### íŒŒì´í”„ë¼ì¸ êµ¬ì„±
```
from preprocessing import  OutlierTransformer, ProportionalImputer, LabelEncoderTransformer, OrdinalEncoderTransformer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder



# Pipelineì„ ì´ìš©í•œ ì „ì²˜ë¦¬
nullvalue_transformer = ColumnTransformer(
    [
        ('income_imputer', ProportionalImputer(), [5]), # income_category
        ('education_imputer', SimpleImputer(strategy='most_frequent'), [3, 4]), # education_level, marital_status
        # ('marital_imputer', SimpleImputer(strategy='most_frequent'), [4]) # marital_status
    ], remainder='passthrough'
)

outlier_transformer = ColumnTransformer(
    [
        ('age_outlier', OutlierTransformer(), [3]), # age
        ('total_trans_outlier', OutlierTransformer(), [16]) # total_trans_cnt
    ], remainder='passthrough'
)
education_categories = [["Uneducated", "High School", "College", "Graduate", "Post-Graduate", "Doctorate"]]
income_categories = [['Less than $40K', '$120K +', '$40K - $60K', '$60K - $80K', '$80K - $120K']]

encoder = ColumnTransformer(
    [
        ('gender_encoder', LabelEncoderTransformer(), [5]), # gender
        ('education_encoder', OrdinalEncoder(categories=education_categories), [3]), # education_level
        ('income_encoder', OrdinalEncoder(categories=income_categories), [2]), # income_category
        ('marital_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [4]), # marital_status
        ('card_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [7]) # card_category
    ], remainder='passthrough'
)

preprocessor_pipeline = Pipeline([
    ("imputer", nullvalue_transformer),
    ("outlier", outlier_transformer),
    ("encoding", encoder),
], verbose=True)
```
#### ë°ì´í„°ì…‹ ì¤€ë¹„
```
from dataloader import load_dataset
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np

# X, y ë¶„ë¦¬
X, y = load_dataset()

# Train/Test/Validation set ë‚˜ëˆ„ê¸°.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)
# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=0)

print(X_train.shape, X_test.shape,y_train.shape, y_test.shape,)
# ë¹„ìœ¨ í™•ì¸
print(np.unique(y, return_counts=True)[1]/y.size)
print(np.unique(y_train, return_counts=True)[1]/y_train.size)
print(np.unique(y_test, return_counts=True)[1]/y_test.size)
print(np.unique(y_valid, return_counts=True)[1]/y_valid.size)
# X_train_preprocessed = preprocessor_pipeline.transform(X_train)
# X_test_preprocessed = preprocessor_pipeline.transform(X_test)

print(X_train_preprocessed.shape)
```
#### ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥
```
import joblib
import os

os.makedirs('models', exist_ok=True)
joblib.dump(
    preprocessor_pipeline,     # ì €ì¥í•  ëª¨ë¸/ì „ì²˜ë¦¬ê¸°
    "models/preprocessor.pkl"  # ì €ì¥ê²½ë¡œ. pickleë¡œ ì €ì¥ëœë‹¤.
)
```
## ëª¨ë¸ë§

### âœ”ï¸ ëª¨ë¸ ì„ ì •í•˜ê¸°

ë°ì´í„°ì™€ ì–´ìš¸ë¦¬ëŠ” 7ê°œì˜ ëª¨ë¸ë“¤ì€ ë½‘ì•„ ì–´ë–¤ ëª¨ë¸ì´ ì í•©í• ì§€ í™•ì¸í•´ ë³´ê¸°ë¡œ í–ˆë‹¤.

- í‰ê°€

```
  from tqdm import tqdm

  from sklearn.linear_model import LogisticRegression
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.ensemble import GradientBoostingClassifier
  from xgboost import XGBClassifier, plot_importance
  from sklearn.svm import SVC
  from sklearn.neighbors import KNeighborsClassifier

  import matplotlib.pyplot as plt
  models = {
      # Logistic Regression model
      "Logistic Regression": LogisticRegression(),
      # Decision Tree model
      "Decision Tree Classifier": DecisionTreeClassifier(),
      # Random Forest model
      "Random Forest": RandomForestClassifier(),
      # Gradient Boosting model
      "Gradient Boosting": GradientBoostingClassifier(),
      # XGBoost model
      "XGBoost": XGBClassifier(),
      # SVM(Support Vector Machine)
      "SVC": SVC(),
      # KNN(K-Nearest Neighbors)
      "KNeighborsClassifier": KNeighborsClassifier(),
  }


  for name, model in tqdm(models.items(), desc="Training Models", total=len(models)):
      # ëª¨ë¸ í›ˆë ¨
      model.fit(X_train, y_train)
      # ëª¨ë¸ í‰ê°€
      score = model.score(X_test, y_test)
      # ëª¨ë¸ ê²€ì¦
      model_pred = model.predict(X_test)
      # ëª¨ë¸ ì •í™•ë„
      tqdm.write(f">>> {name} : ì •í™•ë„ {score:.2%}\n")

```

- ê²°ê³¼

```python
>>> Logistic Regression : ì •í™•ë„ 87.90%

>>> Decision Tree Classifier : ì •í™•ë„ 94.02%

>>> Random Forest : ì •í™•ë„ 95.65%

>>> Gradient Boosting : ì •í™•ë„ 96.15%

>>> XGBoost : ì •í™•ë„ 96.74%

>>> SVC : ì •í™•ë„ 84.20%

>>> KNeighborsClassifier : ì •í™•ë„ 90.47%
```

#### â­ ì„ ì • ê²°ê³¼

- LogisticRegression
- DecisionTreeClassifier (âœ”ï¸) - ê¹€ë™ëª…
- RandomForestClassifier (âœ”ï¸) - ì„ì—°ê²½
- GradientBoostingClassifier (âœ”ï¸) - ë°•ìœ ë‚˜
- xgboost (âœ”ï¸) - ê³µì¸ìš©
- SVC
- KNeighborsClassifier

7ê°œì˜ ëª¨ë¸ ì¤‘ 4ê°œì˜ ëª¨ë¸ì´ ìš°ìˆ˜í•œ í¸ì´ì—ˆê³ , ê°ì ëª¨ë¸ í•œê°œì”© ë§¡ì•„ì„œ ëª¨ë¸ë§ì„ í•˜ê¸°ë¡œ í–ˆë‹¤.

### âœ”ï¸ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸

#### 1. Decision Tree Classifier : ì •í™•ë„ 93.78%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > criterion: ë…¸ë“œ ë¶„í•  ê¸°ì¤€
  >
  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > min_samples_split: ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
  >
  > min_samples_leaf: ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
  >
  > max_features: ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜

  ```

  from sklearn.tree import DecisionTreeClassifier

  # 1. í•™ìŠµ ë° ì˜ˆì¸¡
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)



  tree = DecisionTreeClassifier()

  tree.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_tree = tree.predict(X_train)
  y_train_proba_tree= tree.predict_proba(X_train)[:, 1]

  y_test_pred_tree = tree.predict(X_test)
  y_test_proba_tree= tree.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_tree)
  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - Decision Tree (Test Set)")
  plt.show()

  evaluate("Train - Decision Tree", y_train, y_train_pred_tree, y_train_proba_tree)
  evaluate("Test - Decision Tree", y_test, y_test_pred_tree, y_test_proba_tree)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = tree.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in Decision Tree")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      'criterion': ['gini', 'entropy'],  # ë…¸ë“œ ë¶„í•  ê¸°ì¤€
      'max_depth': [None, 10, 20, 30],   # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      'min_samples_split': [2, 10, 20],  # ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
      'min_samples_leaf': [1, 5, 10],    # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
      'max_features': [None, 'sqrt', 'log2']  # ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜
  }

  gs_tree = GridSearchCV(
      estimator=tree,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_tree.fit(X_train, y_train)

  # 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸
  best_param_tree = gs_tree.best_params_
  best_model_tree = gs_tree.best_estimator_

  best_y_pred_tree = best_model_tree.predict(X_test)
  best_y_proba_tree= best_model_tree.predict_proba(X_test)[:, 1]

  ```

#### 2. Random Forest : ì •í™•ë„ 95.65%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
  >
  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > max_features: ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜

  ```
  from sklearn.ensemble import RandomForestClassifier

  # 1. í•™ìŠµ ë° ì˜ˆì¸¡
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  rf = RandomForestClassifier()

  rf.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_rf = rf.predict(X_train)
  y_train_proba_rf= rf.predict_proba(X_train)[:, 1]

  y_test_pred_rf = rf.predict(X_test)
  y_test_proba_rf= rf.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_rf)
  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - Random Forest (Test Set)")
  plt.show()

  evaluate("Train - Random Forest", y_train, y_train_pred_rf, y_train_proba_rf)
  evaluate("Test - Random Forest", y_test, y_test_pred_rf, y_test_proba_rf)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = rf.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in Random Forest")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      'n_estimators': [100, 200, 300],    # ê²°ì • íŠ¸ë¦¬(Decision Tree)ì˜ ê°œìˆ˜
      'max_depth': [5, 10, 15],           # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      'max_features': ['sqrt', 'log2']    # ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜
  }
  gs_rf = GridSearchCV(
      estimator=rf,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_rf.fit(X_train, y_train)

  # 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸
  best_param_rf = gs_rf.best_params_
  best_model_rf = gs_rf.best_estimator_

  best_y_pred_rf = best_model_rf.predict(X_test)
  best_y_proba_rf= best_model_rf.predict_proba(X_test)[:, 1]

  ```

#### 3. Gradient Boosting : ì •í™•ë„ 96.79%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
  >
  > learning_rate: í•™ìŠµë¥ 
  >
  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > subsample: ê° íŠ¸ë¦¬ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨

  ```
  from sklearn.ensemble import GradientBoostingClassifier

  # 1. í•™ìŠµ ë° ì˜ˆì¸¡
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  gb = GradientBoostingClassifier()

  gb.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_gb = gb.predict(X_train)
  y_train_proba_gb= gb.predict_proba(X_train)[:, 1]

  y_test_pred_gb = gb.predict(X_test)
  y_test_proba_gb= gb.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_gb)
  plt.figure(figsize=(6,4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - Gradient Boosting (Test Set)")
  plt.show()

  evaluate("Train - Gradient Booting", y_train, y_train_pred_gb, y_train_proba_gb)
  evaluate("Test - Gradient Booting", y_test, y_test_pred_gb, y_test_proba_gb)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = gb.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in Gradient Boosting")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      "n_estimators": [100, 200, 300],  #  ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
      "learning_rate": [0.1],  # í•™ìŠµë¥ 
      "max_depth": [1, 2, 3, 4, 5],  # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      "subsample": [0.5, 0.7],  # ìƒ˜í”Œë§ ë¹„ìœ¨
  }

  gs_gb = GridSearchCV(
      estimator=gb,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_gb.fit(X_train, y_train)

  # 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸
  best_param_gb = gs_gb.best_params_
  best_model_gb = gs_gb.best_estimator_

  best_y_pred_gb = best_model_gb.predict(X_test)
  best_y_proba_gb= best_model_gb.predict_proba(X_test)[:, 1]

  ```

#### 4. XGBoost : ì •í™•ë„ 97.19%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > learning_rate: í•™ìŠµë¥ 
  >
  > n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
  >
  > subsample: ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œ ë¹„ìœ¨
  >
  > colsample_bytree: ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” í”¼ì²˜ ë¹„ìœ¨
  >
  > gamma: ë…¸ë“œ ë¶„í• ì— ëŒ€í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ
  >
  > reg_alpha: L1 ì •ê·œí™”
  >
  > reg_lambda: L2 ì •ê·œí™”

  ```
  from xgboost import XGBClassifier

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  xgb = XGBClassifier()

  xgb.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_xgb = xgb.predict(X_train)
  y_train_proba_xgb= xgb.predict_proba(X_train)[:, 1]

  y_test_pred_xgb = xgb.predict(X_test)
  y_test_proba_xgb= xgb.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_xgb)
  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - XGBoost (Test Set)")
  plt.show()

  evaluate("Train - XGBoost", y_train, y_train_pred_xgb, y_train_proba_xgb)
  evaluate("Test - XGBoost", y_test, y_test_pred_xgb, y_test_proba_xgb)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = xgb.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in XGBoost")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      "max_depth":[1, 2, 3, 4, 5],            # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      'learning_rate': [0.1],                 # í•™ìŠµë¥ 
      'n_estimators': [100, 200, 300],        # ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
      'subsample': [0.5, 0.7],                # ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œ ë¹„ìœ¨
      'colsample_bytree': [0.5, 0.7, 1.0],    # ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” í”¼ì²˜ ë¹„ìœ¨
      'gamma': [0, 0.1],                      # ë…¸ë“œ ë¶„í• ì— ëŒ€í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ
      'reg_alpha': [0],                       # L1 ì •ê·œí™”
      'reg_lambda': [0.1]                     # L2 ì •ê·œí™”
  }
  gs_xgb = GridSearchCV(
      estimator=xgb,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_xgb.fit(X_train, y_train)

  # 5. íŠœë‹ : Best Model ì°¾ê¸°
  best_param_xgb = gs_xgb.best_params_
  best_model_xgb = gs_xgb.best_estimator_

  best_y_pred_xgb = best_model_xgb.predict(X_test)
  best_y_proba_xgb= best_model_xgb.predict_proba(X_test)[:, 1]

  ```

| ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•    | Decision Tree Classifier                                                                                                                                                                                                                | Random Forest                                                                                                                                                                                                                                             | Gradient Boosting                                                                                                                                                                                           | XGBoost                                                                                                                                                                                                    |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Confusion Matrix | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-cm.png" alt="image" width="200" height="200"/>                                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-cm.png" width="200" height="200"/>                                                                          | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-cm.png" alt="image" width="200" height="200"/>                                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGboost-cm.png" alt="image" width="200" height="200"/>                                                              |
| ê²°ê³¼             | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-%EA%B2%B0%EA%B3%BC.png" alt="image" width="300" height="150"/>                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%EA%B2%B0%EA%B3%BC.png" width="300" height="150"/>                                                          | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-%EA%B2%B0%EA%B3%BC.png" alt="image" width="300" height="150"/>                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGboost-%EA%B2%B0%EA%B3%BC.png" alt="image" width="300" height="150"/>                                              |
| íŠ¹ì„±ì¤‘ìš”ë„       | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" alt="image" width="300" height="150"/>                   | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" width="300" height="150"/>                               | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" alt="image" width="300" height="150"/>                   | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGboost-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" alt="image" width="300" height="150"/>                   |
| í•˜ì´í¼íŒŒë¼ë¯¸í„°   | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="160"/> | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="100"/> | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="150"/> | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGBoost-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="150"/> |

### âœ”ï¸ ëª¨ë¸ í‰ê°€

```
# ì—¬ëŸ¬ í‰ê°€ ì§€í‘œ ì„¤ì •
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'auc': make_scorer(roc_auc_score)
}

model_box = pd.DataFrame(columns=['decision_tree', 'random_forest', 'gradient_boosting', 'xgboost'],
                            index = ['accuracy','precision','recall','f1 score','auc'])

def evaluate(title, y_real, y_pred, y_prob):
    acc = accuracy_score(y_real, y_pred)
    pre = precision_score(y_real, y_pred)
    rec = recall_score(y_real, y_pred)
    f1 = f1_score(y_real, y_pred)
    auc = roc_auc_score(y_real, y_prob)

    print(f"======= {title} =======")
    print('Accuracy : {:.6f}'.format(acc)) # ì •í™•ë„ : ì˜ˆì¸¡ì´ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€
    print('Precision : {:.6f}'.format(pre)) # ì •ë°€ë„ : ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì—ì„œ ì •ë‹µì˜ ë¹„ìœ¨
    print('Recall : {:.6f}'.format(rec)) # ì¬í˜„ìœ¨ : ì •ë‹µ ì¤‘ì—ì„œ ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨
    print('F1 score : {:.6f}'.format(f1)) # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ (ì¡°í™”)í‰ê·  - ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì´ ë¹„ìŠ·í• ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    print('auc: {:.6f}'.format(auc))


    score_list = [acc,pre,rec,f1,auc]
    score_box = np.array(score_list)

    return score_box
```

![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EC%A0%95%ED%99%95%EB%8F%84.png)

### âœ”ï¸ ìµœê³  ì„±ëŠ¥ ëª¨ë¸

ğŸ† XGBOOST

<br/>

## ëª¨ë¸ ì €ì¥

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ê° ëª¨ë¸ë³„ best params ë¥¼ í†µí•´ ë§Œë“  best modelë“¤ì„ .pkl íŒŒì¼ë¡œ ì €ì¥.

```
import os
import joblib

directory = 'model/'
os.makedirs(directory, exist_ok=True)

joblib.dump(best_model_tree, os.path.join(directory, 'best_tree.pkl'))
joblib.dump(best_model_rf, os.path.join(directory, 'best_rf.pkl'))
joblib.dump(best_model_gb, os.path.join(directory, 'best_gb.pkl'))
joblib.dump(best_model_xgb, os.path.join(directory, 'best_xgb.pkl'))

# ì €ì¥ëœ ëª¨ë¸ê³¼ íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸°
model_tree = joblib.load('model/best_tree.pkl')
model_rf = joblib.load('model/best_rf.pkl')
model_gb = joblib.load('model/best_gb.pkl')
model_xgb = joblib.load('model/best_xgb.pkl')
```

## Streamlit


![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EC%8A%A4%ED%8A%B8%EB%A6%BC%EB%A6%BF%20%EC%8B%A4%ED%96%89%20%ED%99%94%EB%A9%B4.png)

## íŒ€ì› íšŒê³ 
ê¹€ë™ëª…
> 1
>
ë°•ìœ ë‚˜
> ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì¶©ë¶„íˆ ë‹¤ë¤„ë³¼ ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì°¾ëŠ” ë°ì— ì£¼ì•ˆì ì„ ë‘ì—ˆëŠ”ë° íŒ€ì›ë“¤ê³¼ ê°™ì€ ë°©í–¥ì„±ì„ ê³µìœ í•˜ë©° ì ì ˆí•œ ë°ì´í„°ë¥¼ ì„ ì •í•  ìˆ˜ ìˆì—ˆì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤.
>
> ê·¸ë¦¬ê³  ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê³¼ì •ì„ ë‹¤ í•¨ê»˜í•˜ë©° í•™ìŠµí•´ë³´ìëŠ” ëª©í‘œê°€ ìˆì—ˆëŠ”ë° ë¹„ë¡ ì‹œê°„ì´ ìƒëŒ€ì ìœ¼ë¡œ ê±¸ë¦¬ê¸´ í–ˆì§€ë§Œ ê·¸ë§Œí¼ ìœ ìµí•œ ê²½í—˜ì´ë˜ì§€ ì•Šì•˜ë‚˜ ìƒê°í•©ë‹ˆë‹¤. ì €ëŠ” ê·¸ ê³¼ì •ì—ì„œ ë°©í–¥ì„ ì œì‹œí•˜ê³  íŒ€ì›ë“¤ì„ ì´ëŒë©´ì„œ ë…¸ë ¥í–ˆëŠ”ë°, ì´ ê³¼ì •ì—ì„œ ì˜¤íˆë ¤ ì œê°€ ë” ë§ì´ ë°°ì› ë‹¤ê³  ëŠë‚ë‹ˆë‹¤.
>
> íŠ¹íˆ íŒŒì´í”„ë¼ì¸ ì‘ì—… ì¤‘ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì¶œë ¥ìœ¼ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ë²„ê·¸ë¥¼ ë§ˆì£¼í•˜ë©° ë§ì€ ê²ƒì„ ë°°ìš¸ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤. íŒ€ì›ë“¤ì—ê²Œ ì„¤ëª…í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì´ í° ë°°ì›€ì˜ ê¸°íšŒì˜€ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
>
> ë˜í•œ, ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” Streamlitì„ í™œìš©í•˜ì—¬ ì‹œê°í™”ì™€ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ì‘ì—…ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤. ì²˜ìŒ ì˜ˆìƒë³´ë‹¤ ë‹¤ë£¨ê¸° ì‰¬ì› ê³ , í¥ë¯¸ë¡­ê²Œ ë™ì‘í•œë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì„±ì´ ì¢‹ì•„ ìì£¼ í™œìš©í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
>
ì„ì—°ê²½
> ì €ëŠ” íŒ€ì›ë“¤ê³¼ ê°™ì´ ë¨¸ì‹ ëŸ¬ë‹ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë§¡ê³  íŒŒì´í”„ë¼ì¸ ì œì‘ì„ ìœ„í•´ ë…¸ë ¥í–ˆì§€ë§Œ ëë‚´ ì‹¤íŒ¨ í•˜ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì‹¤íŒ¨ ì†ì—ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ëŠ” ëŠì„ì—†ëŠ” ì‹œë„ì™€ í•™ìŠµì€ ì œ ì—­ëŸ‰ì„ ë” ì»¤ì§ˆ ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì£¼ì—ˆê³ , ë‹¤ìŒì—ëŠ” ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¼ ìì‹ ê°ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
>
