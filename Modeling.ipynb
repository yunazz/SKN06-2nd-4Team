{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "## í•œê¸€ ì²˜ë¦¬\n",
    "plt.rcParams['font.family'] = 'malgun gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„°ì…‹ ì½ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/credit_card_churn.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['churn'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹¼ëŸ¼ëª… ìˆ˜ì • + ì†Œë¬¸ìí™”\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = {\n",
    "        'Attrition_Flag': 'churn',\n",
    "        'Customer_Age' : 'age',\n",
    "        'Dependent_count' : 'dependent_cnt',\n",
    "        'Months_on_book' : 'card_usage_period',\n",
    "        'Total_Relationship_Count' : 'account_cnt',\n",
    "        'Months_Inactive_12_mon' : 'inactive_month_in_year',\n",
    "        'Contacts_Count_12_mon' : 'visit_cnt_in_year',\n",
    "        'Total_Revolving_Bal' : 'revolving_balance',\n",
    "        'Avg_Open_To_Buy' : 'avg_remain_credit_limit',\n",
    "        'Total_Amt_Chng_Q4_Q1' : 'total_amt_change_q4_q1',\n",
    "        'Total_Trans_Ct' : 'total_trans_cnt',\n",
    "        'Total_Ct_Chng_Q4_Q1' : 'total_cnt_change_q4_q1'\n",
    "    }\n",
    "data.rename(columns=rename_columns, inplace=True)\n",
    "data.columns = data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¶ˆí•„ìš” ì¹¼ëŸ¼ ì‚­ì œ\n",
    "\n",
    "- clientnum : íšŒì›ë²ˆí˜¸\n",
    "- naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1\n",
    "- naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\n",
    "    columns=[\n",
    "        'clientnum',\n",
    "        'naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1',\n",
    "        'naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2'\n",
    "    ], \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²°ê³¼ê°’ 'churn' mapping\n",
    "\n",
    "- Existing Customer: 0 \n",
    "- Attrited Customer: 1 (ì´íƒˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['churn'] = data['churn'].map({\"Existing Customer\": 0, \"Attrited Customer\": 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê²°ì¸¡ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## í™•ì¸\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•\n",
    "\n",
    "- income_category: ë¹„ìœ¨ì— ë”°ë¥¸ ëŒ€ì¹˜\n",
    "- education_level, marital_status: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ìƒì¹˜\n",
    "\n",
    "### ì´ìƒì¹˜ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR ê¸°ë°˜ìœ¼ë¡œ Outlier ê°’ ì¡°íšŒ ë©”ì†Œë“œ\n",
    "\n",
    "def is_outlier(data, whis=1.5):\n",
    "    \"\"\"\n",
    "    IQR ê¸°ë°˜ìœ¼ë¡œ Outlier ê°’ ì¡°íšŒ ë©”ì†Œë“œ\n",
    "    parameter\n",
    "        data: outlierë¥¼ ì°¾ì„ ë°ì´í„°\n",
    "        whis: IQRì— ëª‡ë°°ë¥¼ ê·¹ë‹¨ì¹˜ ê³„ì‚°ì— ì‚¬ìš©í•  ì§€ ë¹„ìœ¨. rateë¥¼ í¬ê²Œí•˜ë©´ outlierë²”ìœ„ë¥¼ ë„“ê²Œ ì¡ëŠ”ë‹¤. ì‘ê²Œ ì£¼ë©´ ë²”ìœ„ë¥¼ ì¢ê²Œ ì¡ëŠ”ë‹¤.\n",
    "    return\n",
    "        bool type ndarray: ê° ì›ì†Œë³„ outlier ì—¬ë¶€ (True: Outlier(ì´ìƒì¹˜), False: ì •ìƒë²”ìœ„ê°’)\n",
    "    \"\"\"\n",
    "    q1 = np.quantile(data, q=0.25)\n",
    "    q3 = np.quantile(data, q=0.75)\n",
    "    IQR = q3 - q1\n",
    "    return (data < q1 - IQR * whis) | (data > q3 + IQR * whis)\n",
    "\n",
    "\n",
    "def get_normal_range(data, whis=1.5):\n",
    "    \"\"\"\n",
    "    IQR ê¸°ë°˜ìœ¼ë¡œ ì •ìƒë²”ìœ„ ì¡°íšŒ ë©”ì†Œë“œ\n",
    "    parameter1\n",
    "        data: ì¡°íšŒí•  ëŒ€ìƒ ë°ì´í„°\n",
    "        whis: IQRì— ëª‡ë°°ë¥¼ ê·¹ë‹¨ì¹˜ ê³„ì‚°ì— ì‚¬ìš©í•  ì§€ ë¹„ìœ¨. rateë¥¼ í¬ê²Œí•˜ë©´ outlierë²”ìœ„ë¥¼ ë„“ê²Œ ì¡ëŠ”ë‹¤. ì‘ê²Œ ì£¼ë©´ ë²”ìœ„ë¥¼ ì¢ê²Œ ì¡ëŠ”ë‹¤.\n",
    "    return\n",
    "        tuple: (lower_bound, upper_bound) - ì •ìƒë²”ìœ„ì˜ í•˜í•œê°’ê³¼ ìƒí•œê°’\n",
    "    \"\"\"\n",
    "    q1 = np.nanquantile(data, q=0.25)\n",
    "    q3 = np.nanquantile(data, q=0.75)\n",
    "    IQR = q3 - q1\n",
    "    lower_bound = q1 - IQR * whis\n",
    "    upper_bound = q3 + IQR * whis\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê° ì»¬ëŸ¼ë³„ ì´ìƒì¹˜ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age\n",
    "- ë‚˜ì´\n",
    "- ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ ê°’ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.\n",
    "  - ì •ìƒ ë²”ìœ„ë¥¼ ë„˜ì–´ê°„ ê°’ë“¤ì˜ ê°œìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ê°™ì€ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì„œ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_trans_cnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_trans_cnt'].plot(kind='hist', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = get_normal_range(data['total_trans_cnt'], whis=1.5)\n",
    "print(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query('total_trans_cnt > @high').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_trans_cnt\n",
    "- ì´ ê±°ë˜ íšŸìˆ˜\n",
    "- ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ ê°’ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.\n",
    "  - ì •ìƒ ë²”ìœ„ë¥¼ ë„˜ì–´ê°„ ê°’ë“¤ì˜ ê°œìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ê°™ì€ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì„œ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(data['total_trans_cnt'].describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_trans_cnt'].plot(kind='hist', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = get_normal_range(data['total_trans_cnt'], whis=1.5)\n",
    "print(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query('total_trans_cnt > @high').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ ì „ì²˜ë¦¬ ì •ë¦¬\n",
    "- ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "  - income_category: ë¹„ìœ¨ì— ë”°ë¥¸ ëŒ€ì¹˜\n",
    "  - education_level, marital_status: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "\n",
    "- ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "  - age, total_trans_ct\n",
    "    - ì •ìƒë²”ìœ„ì˜ ìµœëŒ€ ê°’ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.  \n",
    "- encoding\n",
    "  - gender \n",
    "    - ë¼ë²¨ ì¸ì½”ë”©(Label Encoding)\n",
    "  - education_level \n",
    "    - ìˆœì„œ ì¸ì½”ë”© (Ordinal Encoding)\n",
    "  - marital_status, card_category \n",
    "    - ì›í•« ì¸ì½”ë”©(One-Hot encoding) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoad í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataloader.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset():\n",
    "    # ë°ì´í„° load\n",
    "    data = pd.read_csv(\"data/credit_card_churn.csv\", na_values=\"Unknown\")\n",
    "\n",
    "    # ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "    rename_columns = {\n",
    "        \"Attrition_Flag\": \"churn\",\n",
    "        \"Customer_Age\": \"age\",\n",
    "        \"Dependent_count\": \"dependent_cnt\",\n",
    "        \"Months_on_book\": \"card_usage_period\",\n",
    "        \"Total_Relationship_Count\": \"account_cnt\",\n",
    "        \"Months_Inactive_12_mon\": \"inactive_month_in_year\",\n",
    "        \"Contacts_Count_12_mon\": \"visit_cnt_in_year\",\n",
    "        \"Total_Revolving_Bal\": \"revolving_balance\",\n",
    "        \"Avg_Open_To_Buy\": \"avg_remain_credit_limit\",\n",
    "        \"Total_Amt_Chng_Q4_Q1\": \"total_amt_change_q4_q1\",\n",
    "        \"Total_Trans_Ct\": \"total_trans_cnt\",\n",
    "        \"Total_Ct_Chng_Q4_Q1\": \"total_cnt_change_q4_q1\",\n",
    "    }\n",
    "    data.rename(columns=rename_columns, inplace=True)\n",
    "    # ì»¬ëŸ¼ëª… ì†Œë¬¸ìë¡œ ë³€ê²½\n",
    "    data.columns = data.columns.str.lower()\n",
    "\n",
    "    ## ì»¬ëŸ¼ ì‚­ì œ\n",
    "    data.drop(\n",
    "        columns=[\n",
    "            \"clientnum\",\n",
    "            \"naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1\",\n",
    "            \"naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    X = data.drop(columns=\"churn\")\n",
    "    y = data[\"churn\"]\n",
    "    y = data['churn'].map({\"Existing Customer\": 0, \"Attrited Customer\": 1})\n",
    "    \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ê¸° ìƒì„±\n",
    "\n",
    ">    - fit() ì€ ì…ë ¥ë°›ì€ ë°ì´í„° X ì™€ y ë¥¼ ì‚¬ìš©í•´ ë³€í™˜í•  ë•Œ ì‚¬ìš©í•  ê°’ì„ ì°¾ì•„ self ì— attributeë¡œ ì €ì¥í•œë‹¤.\n",
    ">    - transform() ì€ fit() ì—ì„œ ì°¾ì€ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "# age, total_trans_coun ì „ì²˜ë¦¬ì— ì ìš©í•  transformer í´ë˜ìŠ¤\n",
    "## - ì •ìƒë²”ìœ„ ìµœëŒ€ê°’, ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "\n",
    "class OutlierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, whis=1.5):\n",
    "        self.whis = whis\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        q1 = np.nanquantile(X, q=0.25)\n",
    "        q3 = np.nanquantile(X, q=0.75)\n",
    "        IQR = q3 - q1\n",
    "        self.lower_bound = q1 - IQR * self.whis\n",
    "        self.upper_bound = q3 + IQR * self.whis\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = np.where(X < self.lower_bound, self.lower_bound, X)\n",
    "        X_transformed = np.where(X_transformed > self.upper_bound, self.upper_bound, X_transformed)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "class ProportionalImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.proportions = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # ê° ì—´ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ì €ì¥\n",
    "        for column in X.columns:\n",
    "            counts = X[column].value_counts(normalize=True, dropna=True)\n",
    "            self.proportions[column] = counts\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for column, probs in self.proportions.items():\n",
    "            # ê²°ì¸¡ì¹˜ ìœ„ì¹˜ ì°¾ê¸°\n",
    "            missing_mask = X[column].isna()\n",
    "            if missing_mask.any():\n",
    "                # ë¹„ìœ¨ì— ë”°ë¼ ëœë¤í•˜ê²Œ ê°’ ì±„ìš°ê¸°\n",
    "                X.loc[missing_mask, column] = np.random.choice(\n",
    "                    probs.index, size=missing_mask.sum(), p=probs.values\n",
    "                )\n",
    "        return X \n",
    "    \n",
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X).reshape(-1, 1)  # 1D ë°°ì—´ì„ 2Dë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "\n",
    "\n",
    "class OrdinalEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categories=[]):\n",
    "        print(categories)\n",
    "        self.encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset()\n",
    "# X = data[0]\n",
    "# y = data[1]\n",
    "# X.columns\n",
    "# for index, column in enumerate(X.columns):\n",
    "#     print(f\"Index: {index}, Column Name: {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## íŒŒì´í”„ë¼ì¸êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocessing import  OutlierTransformer, ProportionalImputer, LabelEncoderTransformer, OrdinalEncoderTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Pipelineì„ ì´ìš©í•œ ì „ì²˜ë¦¬\n",
    "nullvalue_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('income_imputer', ProportionalImputer(), [5]), # income_category\n",
    "        ('education_imputer', SimpleImputer(strategy='most_frequent'), [3, 4]), # education_level, marital_status\n",
    "        # ('marital_imputer', SimpleImputer(strategy='most_frequent'), [4]) # marital_status\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "outlier_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('age_outlier', OutlierTransformer(), [3]), # age\n",
    "        ('total_trans_outlier', OutlierTransformer(), [16]) # total_trans_cnt\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "education_categories = [[\"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"]]\n",
    "income_categories = [['Less than $40K', '$120K +', '$40K - $60K', '$60K - $80K', '$80K - $120K']]\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "    [\n",
    "        ('gender_encoder', LabelEncoderTransformer(), [5]), # gender\n",
    "        ('education_encoder', OrdinalEncoder(categories=education_categories), [3]), # education_level\n",
    "        ('income_encoder', OrdinalEncoder(categories=income_categories), [2]), # income_category\n",
    "        ('marital_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [4]), # marital_status\n",
    "        ('card_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [7]) # card_category\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor_pipeline = Pipeline([\n",
    "    (\"imputer\", nullvalue_transformer),\n",
    "    (\"outlier\", outlier_transformer),\n",
    "    (\"encoding\", encoder),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„°ì…‹ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8101, 19) (2026, 19) (8101,) (2026,)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# X, y ë¶„ë¦¬\n",
    "X, y = load_dataset()\n",
    "\n",
    "# Train/Test/Validation set ë‚˜ëˆ„ê¸°.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape,y_train.shape, y_test.shape,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83934038 0.16065962]\n",
      "[0.83934156 0.16065844]\n",
      "[0.83958539 0.16041461]\n",
      "[0.83909181 0.16090819]\n"
     ]
    }
   ],
   "source": [
    "# ë¹„ìœ¨ í™•ì¸\n",
    "print(np.unique(y, return_counts=True)[1]/y.size)\n",
    "print(np.unique(y_train, return_counts=True)[1]/y_train.size)\n",
    "print(np.unique(y_test, return_counts=True)[1]/y_test.size)\n",
    "print(np.unique(y_valid, return_counts=True)[1]/y_valid.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8101, 24)\n"
     ]
    }
   ],
   "source": [
    "# X_train_preprocessed = preprocessor_pipeline.transform(X_train)\n",
    "# X_test_preprocessed = preprocessor_pipeline.transform(X_test)\n",
    "\n",
    "print(X_train_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/preprocessor.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(\n",
    "    preprocessor_pipeline,     # ì €ì¥í•  ëª¨ë¸/ì „ì²˜ë¦¬ê¸°\n",
    "    \"models/preprocessor.pkl\"  # ì €ì¥ê²½ë¡œ. pickleë¡œ ì €ì¥ëœë‹¤.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline ëª¨ë¸ë§\n",
    "\n",
    "## Baseline ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models\\\\best_xgb.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'auc': make_scorer(roc_auc_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_train_pred_tree = tree.predict(X_train_preprocessed)\n",
    "y_train_proba_tree= tree.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],  # ë…¸ë“œ ë¶„í•  ê¸°ì¤€\n",
    "    'max_depth': [None, 10, 20, 30],   # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •\n",
    "    'min_samples_split': [2, 10, 20],  # ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n",
    "    'min_samples_leaf': [1, 5, 10],    # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n",
    "    'max_features': [None, 'sqrt', 'log2']  # ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(\n",
    "    estimator=tree,          \n",
    "    param_grid=params,  \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,             \n",
    "    n_jobs=-1,         \n",
    ")\n",
    "\n",
    "gs_tree.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "best_param_tree = gs_tree.best_params_\n",
    "best_model_tree = gs_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\ml\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# 4-2-2. Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_train_pred_rf = rf.predict(X_train_preprocessed)\n",
    "y_train_proba_rf= rf.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 300],    # ê²°ì • íŠ¸ë¦¬(Decision Tree)ì˜ ê°œìˆ˜\n",
    "    'max_depth': [5, 10, 15],           # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •\n",
    "    'max_features': ['sqrt', 'log2']    # ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜\n",
    "}\n",
    "gs_rf = GridSearchCV(\n",
    "    estimator=rf,       \n",
    "    param_grid=params,     \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,                      \n",
    "    n_jobs=-1,             \n",
    ")\n",
    "\n",
    "gs_rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸\n",
    "best_param_rf = gs_rf.best_params_\n",
    "best_model_rf = gs_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\"\"\"\n",
    "====================================================================================\n",
    "ì£¼ìš” íŒŒë¼ë¯¸í„°\n",
    "=======================================================a=============================\n",
    "- n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜\n",
    "- learning_rate: í•™ìŠµë¥ \n",
    "- max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •\n",
    "- subsample: ê° íŠ¸ë¦¬ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "gb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 2. ëª¨ë¸ í‰ê°€\n",
    "# Train set + Test set í‰ê°€\n",
    "y_train_pred_gb = gb.predict(X_train_preprocessed)\n",
    "y_train_proba_gb= gb.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300],  #  ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    \"learning_rate\": [0.1],  # í•™ìŠµë¥ \n",
    "    \"max_depth\": [1, 2, 3, 4, 5],  # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •\n",
    "    \"subsample\": [0.5, 0.7],  # ìƒ˜í”Œë§ ë¹„ìœ¨\n",
    "}\n",
    "\n",
    "gs_gb = GridSearchCV(\n",
    "    estimator=gb,           \n",
    "    param_grid=params,   \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,                  \n",
    "    n_jobs=-1,            \n",
    ")\n",
    "\n",
    "gs_gb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "best_param_gb = gs_gb.best_params_\n",
    "best_model_gb = gs_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "  \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 2. ëª¨ë¸ í‰ê°€\n",
    "# Train set + Test set í‰ê°€\n",
    "y_train_pred_xgb = xgb.predict(X_train_preprocessed)\n",
    "y_train_proba_xgb= xgb.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "fi = xgb.feature_importances_\n",
    "\n",
    "\n",
    "# 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV\n",
    "params = {\n",
    "    \"max_depth\":[1, 2, 3, 4, 5],            # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •\n",
    "    'learning_rate': [0.1],                 # í•™ìŠµë¥ \n",
    "    'n_estimators': [100, 200, 300],        # ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    'subsample': [0.5, 0.7],                # ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œ ë¹„ìœ¨\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0],    # ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” í”¼ì²˜ ë¹„ìœ¨\n",
    "    'gamma': [0, 0.1],                      # ë…¸ë“œ ë¶„í• ì— ëŒ€í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ\n",
    "    'reg_alpha': [0],                       # L1 ì •ê·œí™”\n",
    "    'reg_lambda': [0.1]                     # L2 ì •ê·œí™”\n",
    "}\n",
    "gs_xgb = GridSearchCV(\n",
    "    estimator=xgb,           \n",
    "    param_grid=params,   \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,                  \n",
    "    n_jobs=-1,            \n",
    ")\n",
    "\n",
    "gs_xgb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 5. íŠœë‹ : Best Model ì°¾ê¸°\n",
    "best_param_xgb = gs_xgb.best_params_\n",
    "best_model_xgb = gs_xgb.best_estimator_\n",
    "\n",
    "best_y_pred_xgb = best_model_xgb.predict(X_test_preprocessed)\n",
    "best_y_proba_xgb= best_model_xgb.predict_proba(X_test_preprocessed)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ì €ì¥, ìµœì¢… í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'models/'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model_tree, os.path.join(directory, 'best_tree.pkl'))\n",
    "joblib.dump(best_model_rf, os.path.join(directory, 'best_rf.pkl'))\n",
    "joblib.dump(best_model_gb, os.path.join(directory, 'best_gb.pkl'))\n",
    "joblib.dump(best_model_xgb, os.path.join(directory, 'best_xgb.pkl'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
